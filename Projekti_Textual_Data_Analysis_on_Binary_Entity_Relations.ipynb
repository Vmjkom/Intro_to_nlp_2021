{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Projekti Textual Data Analysis on Binary Entity Relations.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vmjkom/Intro_to_nlp_2021/blob/main/Projekti_Textual_Data_Analysis_on_Binary_Entity_Relations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdfpgeo4vSs3"
      },
      "source": [
        "# Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCxSDcuRcp-P"
      },
      "source": [
        "In this notebook implement a machine learning-based information extraction system that identifies binary relations stated to hold between specific named entities mentioned in the same sentence (e.g. PERSON founded ORGANIZATION). \n",
        "\n",
        "The projects is divided into three milestones:\n",
        "\n",
        " * Milestone 1: Manual relation annotation using texts and tool \n",
        "provided by course organizers.\n",
        "\n",
        " * Milestone 2: Implementation of machine-learning based method for relation extraction. \n",
        "\n",
        " * Milestone 3: Large-scale application, assessment and and error analysis of method implemented in milestone 2.\n",
        "\n",
        "Each group is expected to prepare a report on their work. Reports can be in any broadly supported format (e.g. PDF or Python notebook) and should include both the code implementing the project (separate .py files are OK) as well as a textual description of the project and its results.\n",
        "\n",
        "The contribution of each member of the group should be varied across all the facets of the project, and the project report should include a statement on what the contribution of each member towards each milestone was.\n",
        "\n",
        "Additional guidance from Discord:\n",
        "\n",
        "\"For milestone 2, the minimum for \"completing\" will be outperforming the majority baseline, which will hopefully not be particularly challenging. For the additional point, going the extra mile in terms like considering multiple metrics, models, or **input representations** and/or doing hyperparameter optimization count. For milestone 3, the minimum for \"complete\" is successfully applying the model to a larger set of new data and reporting some meaningful descriptive metrics about the result, and for the extra point e.g. error analysis or comparing different model variants will suffice. Basically, if you've successfully completed all milestones, can show effort beyond the minimum at each, and have written up your work in a reasonable way, you can expect to receive full points\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvd_0QX6dQ-4"
      },
      "source": [
        "# imports \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8jjSxZszfnp"
      },
      "source": [
        "!pip -q install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKjG5UlObCZs"
      },
      "source": [
        "### Relation extraction system task formulation\n",
        "\n",
        "This description of ML task formulation is from UTU Textual Data analysis [course material](https://colab.research.google.com/github/TurkuNLP/Text_Mining_Course/blob/master/Relation%20extraction.ipynb#scrollTo=PKjG5UlObCZs)\n",
        "\n",
        "Given manually annotated examples where each consists of\n",
        "\n",
        "* Document text $d$\n",
        "* Two entity mentions in that document, $m_1$ and $m_2$\n",
        "* Relation type (e.g. `employee`) or `NONE` to signify no relation\n",
        "\n",
        "relation extraction can be formulated as a classification task by creating an representation of the mentions $m_1$ and $m_2$ in their context ($d$) as input and the relation type (or `NONE`) as output. (Note that this assumes at most one relation type holds per entity pair.)\n",
        "\n",
        "In previous state-of-the-art approaches, considerable effort was invested into creating representations of the mentions in context for ML methods, frequently involving e.g. carefully engineered representations of dependency paths.\n",
        "\n",
        "Fortunately for us, with recent Transformer-based approaches such as BERT, these representations can be simplified into marking the entities in the original text in some way, and providing the text with marked entities to the model as input.\n",
        "\n",
        "For example, given\n",
        "\n",
        "* Document text $d$ = `Bill Gates and Paul Allen founded Microsoft`\n",
        "* Mentions $m_1$ = `(0, 10, PERSON, Bill Gates)` and $m_2$ = `(34, 43, ORG, Microsoft)`\n",
        "* Relation type `founder`\n",
        "\n",
        "We could formulate the classification example e.g. as\n",
        "\n",
        "* input: `PERSON and Paul Allen founded ORGANIZATION`\n",
        "* output: `founder`\n",
        "\n",
        "Here using the literal type strings `PERSON` and `ORGANIZATION` to mark the two mentions under consideration. (Note that other mentions in context are not marked.)\n",
        "\n",
        "(**Project work** As the course project involves implementing a relation extraction system along the lines outlined above, we will not provide an implementation here. You may find the previously covered notebooks on text classification helpful as references for your implementation.)\n",
        "\n",
        "Machine learning means the \"rules\" for solving the given task are learned from examples, they are not explicitly written."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxxJebYYvIce"
      },
      "source": [
        "# MS 2 starts here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "498_bEVsdDOL"
      },
      "source": [
        "!wget -q -nc http://dl.turkunlp.org/TKO_8964_2021/PER-ORG-relations-combined-exercise.tsv"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3zJjNjpdrFU"
      },
      "source": [
        "#pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "def open_file(file):\n",
        "  df = pd.read_csv(file, sep='\\t', header=None)\n",
        "  df = df.rename(columns={0: 'id', 1: 'label', 2: 'document'})\n",
        "  df = df.drop('id', axis=1)\n",
        "  return df\n",
        "\n",
        "data = open_file('PER-ORG-relations-combined-exercise.tsv')\n",
        "# data.head(5)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKgpi_QPBdWH"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_V3_-M9ChY_",
        "outputId": "1fef51c3-d4dd-429e-bc7f-b5e6ed0d6674"
      },
      "source": [
        "print(train_test_valid_dataset)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'document'],\n",
            "        num_rows: 2169\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'document'],\n",
            "        num_rows: 302\n",
            "    })\n",
            "    valid: Dataset({\n",
            "        features: ['label', 'document'],\n",
            "        num_rows: 301\n",
            "    })\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL99JUU1e5TL"
      },
      "source": [
        "# Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZEULB8Twh-TH",
        "outputId": "8ae9e2f9-7e91-43af-8b96-38f39c1dbd02"
      },
      "source": [
        "data.groupby(['label']).count()\n",
        "\n",
        "sns.countplot(data=data, x='label');"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEY5me93jecr"
      },
      "source": [
        "Some words about the data quality?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avbs3UKGe4-n",
        "outputId": "2cbfe88e-55f5-48b2-8737-ec8b613383df"
      },
      "source": [
        "# We have these classes in the data\n",
        "labels=(data['label'])\n",
        "print(type(labels))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q16OKRd4fIXS"
      },
      "source": [
        "for i in range(10):\n",
        "  print(data['label'][i], '\\t', data['document'][i])\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIIHOjopgTSG"
      },
      "source": [
        "In the input data \"documents\" we can see the tags signaling named entities:\n",
        "\n",
        "\n",
        "*   `<ORG> ORGANISATION </ORG>`\n",
        "*   `<PERSON> PERSON </PERSON>`\n",
        "\n",
        "These NER (Named Entity Recognition) tags are used in language modeling to recognize permanent entities and relations between them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1UaZ4lx1cnI"
      },
      "source": [
        "#Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-R3edfDc05_"
      },
      "source": [
        "## Handilng the tags and the tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxklF3hhdCoA"
      },
      "source": [
        "### Approach 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCr4mDZTeA1p",
        "outputId": "e46f7f9f-308b-4927-f613-aef88c4571b8"
      },
      "source": [
        "import re\n",
        "\n",
        "pattern_person = r\"<PERSON>.+</PERSON>\" # .+ Matches one or more any charachter\n",
        "docs = [re.sub(pattern_person, \"PERSON\", item) for item in data['document']]\n",
        "\n",
        "pattern_org = r\"<ORG>.+</ORG>\" # .+ Matches one or more any charachter\n",
        "docs = [re.sub(pattern_org, \"ORG\", item) for item in docs]\n",
        "\n",
        "for i in range(0,200,100):\n",
        "  print(data['document'][i])\n",
        "  print(docs[i])\n",
        "  print()\n",
        "  # break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wedell 's company , <ORG>the Wedell - Williams Air Service Corporation</ORG> , won 14 `` distinguished finishes `` ( top five ) in the <PERSON>Thompson</PERSON> and Bendix Trophy races .\n",
            "Wedell 's company , ORG , won 14 `` distinguished finishes `` ( top five ) in the PERSON and Bendix Trophy races .\n",
            "\n",
            "In 1949 , <PERSON>Bardot</PERSON> was accepted at <ORG>the Conservatoire de Paris</ORG> .\n",
            "In 1949 , PERSON was accepted at ORG .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z4HXDoDcIm-"
      },
      "source": [
        "# for i in data['document'][1]:\n",
        "#   print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efq3y7zX99ur"
      },
      "source": [
        "### Approach 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGEsiVwq-DF4"
      },
      "source": [
        "The data needs to be something like?: \n",
        "\n",
        "`'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']`\n",
        "\n",
        "`'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68Ekl4nG-F6Y",
        "outputId": "3cba08a7-3ecd-4c80-84ec-a777271bc9ee"
      },
      "source": [
        "one = data['document'][4]\n",
        "print(one)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Lima-based newspaper `` Perú 21 `` ran an editorial noting that even though the <ORG>Universidad de Lima</ORG> poll results indicate that four out of every five interviewees believe that <PERSON>Fujimori</PERSON> is guilty of some of the charges against him , he still enjoys at least 30 % of popular support and enough approval to restart a political career .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUUMqg42-Nkz",
        "outputId": "11b33a04-8a27-4d7f-a6b9-93d0b77ddfec"
      },
      "source": [
        "org_start=0\n",
        "org_end=0\n",
        "\n",
        "per_start=0\n",
        "per_end=0\n",
        "\n",
        "tokens=one.split(\" \")\n",
        "tags = [\"O\"] * len(tokens) # initialize tags: all O at the start\n",
        "print(tokens)\n",
        "\n",
        "for i, t in enumerate(tokens):\n",
        "  if t.startswith(\"<ORG>\"):\n",
        "    org_start=i # starting index (B) of multitoken ORG tag\n",
        "    tags[i]=\"B_ORG\" # replace a tag \n",
        "    tokens[i]=re.sub(r\"<ORG>\", \"\", t) # remove tag from the tokens\n",
        "  if t.endswith(\"</ORG>\"):\n",
        "    org_end=i # ending index of multitoken ORG tag\n",
        "    tokens[i]=re.sub(r\"</ORG>\", \"\", t) # remove tag from the tokens\n",
        "  # all the same but for PER tags instead of ORG tags\n",
        "  if t.startswith(\"<PERSON>\"):\n",
        "    per_start=i\n",
        "    tags[i]=\"B_PER\"\n",
        "    tokens[i]=re.sub(r\"<PERSON>\", \"\", t)\n",
        "  if t.endswith(\"</PERSON>\"):\n",
        "    per_end=i\n",
        "    tokens[i]=re.sub(r\"</PERSON>\", \"\", t)\n",
        "\n",
        "# use the indices collected above to set the in (I) tags\n",
        "org_indexes=list(range(org_start+1, org_end+1))\n",
        "if not org_indexes: # if no indices (i.e. token to be tagged is single)\n",
        "  tokens[org_start]=re.sub(r\"<ORG>\", \"\", tokens[org_start]) # just fix the token\n",
        "else:\n",
        "  for index in org_indexes:\n",
        "    tags[index] = \"I_ORG\"\n",
        "\n",
        "# all the same but for PER tags instead of ORG tags\n",
        "per_indexes=list(range(per_start+1, per_end+1))\n",
        "if not per_indexes: \n",
        "  tokens[per_start]=re.sub(r\"<PERSON>\", \"\", tokens[per_start])\n",
        "for index in per_indexes:\n",
        "    tags[index] = \"I_PER\"\n",
        "\n",
        "for tag, token in zip(tags, tokens):\n",
        "  print(tag, \"\\t\",token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Lima-based', 'newspaper', '``', 'Perú', '21', '``', 'ran', 'an', 'editorial', 'noting', 'that', 'even', 'though', 'the', '<ORG>Universidad', 'de', 'Lima</ORG>', 'poll', 'results', 'indicate', 'that', 'four', 'out', 'of', 'every', 'five', 'interviewees', 'believe', 'that', '<PERSON>Fujimori</PERSON>', 'is', 'guilty', 'of', 'some', 'of', 'the', 'charges', 'against', 'him', ',', 'he', 'still', 'enjoys', 'at', 'least', '30', '%', 'of', 'popular', 'support', 'and', 'enough', 'approval', 'to', 'restart', 'a', 'political', 'career', '.']\n",
            "O \t The\n",
            "O \t Lima-based\n",
            "O \t newspaper\n",
            "O \t ``\n",
            "O \t Perú\n",
            "O \t 21\n",
            "O \t ``\n",
            "O \t ran\n",
            "O \t an\n",
            "O \t editorial\n",
            "O \t noting\n",
            "O \t that\n",
            "O \t even\n",
            "O \t though\n",
            "O \t the\n",
            "B_ORG \t Universidad\n",
            "I_ORG \t de\n",
            "I_ORG \t Lima\n",
            "O \t poll\n",
            "O \t results\n",
            "O \t indicate\n",
            "O \t that\n",
            "O \t four\n",
            "O \t out\n",
            "O \t of\n",
            "O \t every\n",
            "O \t five\n",
            "O \t interviewees\n",
            "O \t believe\n",
            "O \t that\n",
            "B_PER \t Fujimori\n",
            "O \t is\n",
            "O \t guilty\n",
            "O \t of\n",
            "O \t some\n",
            "O \t of\n",
            "O \t the\n",
            "O \t charges\n",
            "O \t against\n",
            "O \t him\n",
            "O \t ,\n",
            "O \t he\n",
            "O \t still\n",
            "O \t enjoys\n",
            "O \t at\n",
            "O \t least\n",
            "O \t 30\n",
            "O \t %\n",
            "O \t of\n",
            "O \t popular\n",
            "O \t support\n",
            "O \t and\n",
            "O \t enough\n",
            "O \t approval\n",
            "O \t to\n",
            "O \t restart\n",
            "O \t a\n",
            "O \t political\n",
            "O \t career\n",
            "O \t .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GZPM_3EdHSg"
      },
      "source": [
        "### Approach 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59u3Y8uxAjiB"
      },
      "source": [
        "The data needs to be something like?: \n",
        "\n",
        "`'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']`\n",
        "\n",
        "`'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edWHl0jPaavY"
      },
      "source": [
        "Any hints for preparing the data?\n",
        "\n",
        "FinBERT was trained with:\n",
        "\n",
        "Teemu Ruokolainen, Pekka Kauppinen, Miikka Silfverberg,\n",
        "and Krister Lind´en. 2019. A Finnish news\n",
        "corpus for named entity recognition. Language Resources\n",
        "and Evaluation, pages 1–26.\n",
        "\n",
        "Link to the article: https://arxiv.org/pdf/1908.04212.pdf\n",
        "\n",
        "available at: https://github.com/mpsilfve/finer-data\n",
        "\n",
        "Tags: BIO + NER:\n",
        "\n",
        "Irlannin\tB-ORG\n",
        "\n",
        "hallitus\tI-ORG\t\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psh70WHwdo3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de49e293-ed57-47b6-9365-f6d66f6284d9"
      },
      "source": [
        "# # import re\n",
        "\n",
        "# # print(data['document'][0])\n",
        "\n",
        "# per_start = r\"<PERSON>\" \n",
        "# testing = [re.sub(per_start, \"<PERSON> \", item) for item in data['document']]\n",
        "# per_end = r\"</PERSON>\" \n",
        "# testing = [re.sub(per_end, \" </PERSON>\", item) for item in testing]\n",
        "\n",
        "# org_start = r\"<ORG>\" \n",
        "# testing = [re.sub(org_start, \"<ORG> \", item) for item in testing]\n",
        "# org_end = r\"</ORG>\" \n",
        "# testing = [re.sub(org_end, \" </ORG>\", item) for item in testing]\n",
        "\n",
        "# # pattern_org = r\"<ORG>.+</ORG>\" # .+ Matches one or more any charachter\n",
        "# # testing = [re.sub(pattern_org, \"ORG\", item) for item in testing]\n",
        "\n",
        "# for line in testing:\n",
        "#   tags=[]\n",
        "#   tokens=[]\n",
        "#   words = line.split(\" \")\n",
        "#   print(words)\n",
        "#   for index, word in enumerate(words):\n",
        "#     if word == \"<PERSON>\":\n",
        "#       tags.append(\"B_PER\")\n",
        "#     # if word != \"</PERSON>\":\n",
        "#     #   tags.append(\"I_PER\")\n",
        "#       while word != \"</PERSON>\":\n",
        "#         tags.append(\"I_PER\")\n",
        "#         tokens.append(word)\n",
        "#     if word is \"</PERSON>\":\n",
        "#       continue   \n",
        "#     if word is \"<ORG>\":\n",
        "#       tags.append(\"B_ORG\")\n",
        "#       while word is not \"</ORG>\":\n",
        "#         tags.append(\"I_ORG\")\n",
        "#         tokens.append(word)\n",
        "#     else: \n",
        "#       tags.append(\"O\")\n",
        "#       tokens.append(word)\n",
        "#     # if word not in [\"<PERSON>\", \"</PERSON>\", \"<ORG>\", \"</ORG>\"]:\n",
        "#     #   print(word)\n",
        "#     #   tags.append(\"O\")\n",
        "  \n",
        "#   break\n",
        "\n",
        "# for token, tag in zip(tokens, tags):\n",
        "#   print(tag, \"\\t\", token)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Wedell', \"'s\", 'company', ',', '<ORG>', 'the', 'Wedell', '-', 'Williams', 'Air', 'Service', 'Corporation', '</ORG>', ',', 'won', '14', '``', 'distinguished', 'finishes', '``', '(', 'top', 'five', ')', 'in', 'the', '<PERSON>', 'Thompson', '</PERSON>', 'and', 'Bendix', 'Trophy', 'races', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nJczF6hTgKz"
      },
      "source": [
        "\n",
        "# tags=[]\n",
        "# tokens=[]\n",
        "\n",
        "# # list.insert(index, elem) -- inserts the element at the given index, shifting elements to the right.\n",
        "# line=testing[0]\n",
        "\n",
        "# words = line.split(\" \")\n",
        "# # keep=words.copy() # remove form this\n",
        "# print(words)\n",
        "# print()\n",
        "# for index, word in enumerate(words):\n",
        "#   if word == \"<ORG>\":\n",
        "#     # del keep[index]\n",
        "#     tags.append(\"B_ORG\")\n",
        "#   if word == \"</ORG>\":\n",
        "#     # del keep[index]\n",
        "#     # tags.append(\"B_ORG\")\n",
        "#     # continue\n",
        "#   else: \n",
        "#     tags.append(\"O\")\n",
        "#     tokens.append(word)\n",
        "# # print(tags)\n",
        "# # print(tokens)\n",
        "\n",
        "# for token, tag in zip(tokens, tags):\n",
        "#   print(tag, \"\\t\", token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gwwsKdQ3Oap"
      },
      "source": [
        "### UDPipe?\n",
        "\n",
        "- Machine learned tokenizer and segmenter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHtWHV1YcZk5"
      },
      "source": [
        "# %%bash\n",
        "# wget -nc -q https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "# #wget -nc https://github.com/UniversalDependencies/UD_Finnish-TDT v.2.2\n",
        "# pip3 install ufal.udpipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si0GVDvucC0h"
      },
      "source": [
        "\n",
        "# # Documentation: https://ufal.mff.cuni.cz/udpipe/users-manual\n",
        "# import ufal.udpipe as udpipe\n",
        "\n",
        "# model = udpipe.Model.load(\"en.segmenter.udpipe\")\n",
        "# pipeline = udpipe.Pipeline(model,\"tokenize\",\"none\",\"none\",\"horizontal\") # horizontal: returns one sentence per line, with words separated by a single space\n",
        "\n",
        "# segmented_document = pipeline.process(test_input)\n",
        "\n",
        "# print(segmented_document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHe521w6dkB6"
      },
      "source": [
        "Nope, not like this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0vmDy-2i6ck"
      },
      "source": [
        "## Train test split \n",
        "\n",
        "https://github.com/HannaKi/Textual-Data-Analysis-UTU/blob/master/train_bert_for_ner_exercise.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98Q0c7nw0G5k"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(docs, data['label'], test_size=0.33) # treeni ja testidata\n",
        "\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.33) # jaetaan testidata vielä kertaalleen, jotta saadaan validointdata"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A01TAm8ECSp",
        "outputId": "8dcedc98-046e-4344-9cbd-c10c10fb571d"
      },
      "source": [
        "print(type(data))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUMaN_XhDb85"
      },
      "source": [
        "panda = pd.DataFrame(list(zip(docs, labels)),\n",
        "               columns =['text', 'label'])\n",
        "\n",
        "#temp = pd.Dataframe(data=docs)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cm_mBES-6oy"
      },
      "source": [
        "from datasets import Dataset,DatasetDict\n",
        "dataset = Dataset.from_pandas(panda)\n",
        "\n",
        "#\n",
        "train_test = dataset.train_test_split(test_size=0.3)\n",
        "testi_data = train_test['test'].train_test_split(test_size=0.2)\n",
        "train_test_valid_dataset = DatasetDict({\n",
        "    'train': train_test['train'],\n",
        "    'development': testi_data['test'],\n",
        "    'test': testi_data['train']})\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmiARdcFE3g9",
        "outputId": "586d12c6-1269-4492-9c69-46d08afdfef3"
      },
      "source": [
        "print(train_test_valid_dataset)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1687\n",
            "    })\n",
            "    development: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 145\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 579\n",
            "    })\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfCBtt46GQet"
      },
      "source": [
        "for teksti in "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM_dnRHV1kJ9",
        "outputId": "c38efb59-e54a-42a5-ab53-b913e006b08f"
      },
      "source": [
        "print(len(X_train), len(X_test), len(X_val))\n",
        "\n",
        "print(len(y_train), len(y_test), len(y_val))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1615 533 263\n",
            "1615 533 263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjQ38kYS6tzl",
        "outputId": "0a6ff094-7454-476e-9923-e172172fe267"
      },
      "source": [
        "for rivi in X_train[:5]:\n",
        "  print(rivi)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In December 1946 , ORG ( DST ) , led by commando and counter - insurgency expert Captain PERSON , were accused of pacifying the southern Sulawesi region using arbitrary terror techniques , which were copied by other anti-Republicans .\n",
            "In recent years , PERSON hosted the ORG baseball tournament ( 2001–2009 ) and the National Club Baseball Association World Series ( 2015 and 2016 ) .\n",
            "In 1612 , PERSON introduced ORG , a constellation representing a bee .\n",
            "Other probable sources of inspiration for `` Candide `` are ( 1699 ) by PERSON and ( 1753 ) by ORG .\n",
            "In mainland China , People 's Republic of China was established by ORG , with PERSON as its state chairman .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf8mT6mXdRBX"
      },
      "source": [
        "## Data to datasets.Dataset format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zbY9OLv2Grb"
      },
      "source": [
        "!pip --quiet install datasets"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2f3ZMsuUfjA"
      },
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "id=[1,2]\n",
        "texts=[]\n",
        "tags=[]\n",
        "\n",
        "t1=['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
        "texts.append(t1)\n",
        "t2=['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']\n",
        "texts.append(t2)\n",
        "\n",
        "tag1=[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
        "tags.append(tag1)\n",
        "tag2=[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n",
        "tags.append(tag1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjpK2nlCXMFs"
      },
      "source": [
        "m_dict={}\n",
        "m_dict['id']=id\n",
        "m_dict['texts']=texts\n",
        "m_dict['tags']=tags"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im1BavNd3oN_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "ca7a597c-9ceb-4826-fa06-6059985c5ddd"
      },
      "source": [
        "splitit={}\n",
        "splits['train']="
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-73dfd946716c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    splits['train']=\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiTbsaxNWe-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d43335c-587d-430e-f01c-3db9ffa29415"
      },
      "source": [
        "dat = Dataset.from_dict(m_dict)\n",
        "print(dat)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'texts', 'tags'],\n",
            "    num_rows: 2\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf_R3aYC38bM"
      },
      "source": [
        "## Load tokenizer and tokenize data\n",
        "\n",
        "We'll load an appropriate tokenizer for the pre-trained model with [`AutoTokenizer.from_pretrained`](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer.from_pretrained). \n",
        "\n",
        "!!!(We request a [\"fast\" tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html) as it provides a mapping from tokens to input words, which we need for NER.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD0w7-DOrm05"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME),# use_fast=True) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUFrEj5f8ca3"
      },
      "source": [
        "def encode_dataset(d):\n",
        "  return tokenizer(d['sentence'])\n",
        "\n",
        "encoded_dataset = dataset.map(encode_dataset) #tokenize all of the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL5g1p1vG1pP"
      },
      "source": [
        "tokenizer(test_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHcRvjRK3ipT"
      },
      "source": [
        "Tokenze all the splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QC2BG-_1tlT"
      },
      "source": [
        "## Model preparation stuff here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poZJZzmAi5vY"
      },
      "source": [
        "!pip --quiet install transformers\n",
        "!pip --quiet install datasets\n",
        "!pip --quiet install seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT_IXPG7r4-R"
      },
      "source": [
        "Find a Finnish BERT model in the Hugging Face model repository and the Turku NER corpus in the dataset repository.\n",
        "\n",
        "Take care that the model is case-sensitive (cased or not). \n",
        "\n",
        "Case sensitive: words starting with capital letters are not the same as words with lower case: House != house)\n",
        "    \n",
        "- Model: TurkuNLP/bert-base-finnish-cased-v1: https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1\n",
        "- Data: turku_ner_corpus https://huggingface.co/datasets/turku_ner_corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt3hRKGUrGUN"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "\n",
        "MODEL_NAME = 'TurkuNLP/bert-base-finnish-cased-v1'\n",
        "DATASET = 'turku_ner_corpus'\n",
        "MAX_LENGTH=128 # ?\n",
        "DUMMY_LABEL_ID = -100    # Don't change this!\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    'output_dir',\n",
        "    save_strategy='no',\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_strategy='epoch',\n",
        "    learning_rate=1e-5, # --> 0.00001\n",
        "    per_device_train_batch_size=32, # 4\n",
        "    num_train_epochs=2, #1\n",
        ")\n",
        "\n",
        "# complementary things: increase learning rate, decrease batch size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5PK2xKTrRhn"
      },
      "source": [
        "What should the data set look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpL7FMY94FnP"
      },
      "source": [
        "from datasets import load_dataset, load_metric\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhUpfOvCrQ4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "3beb9fd4-f187-4420-8cc6-f95f5f0d054f"
      },
      "source": [
        "dataset = load_dataset(DATASET)\n",
        "label_list = dataset[\"train\"].features['ner_tags'].feature.names\n",
        "num_labels = len(label_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4c79257ffc34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ner_tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DATASET' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSb4QpRGtDvR"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N1fFBKp3Mni"
      },
      "source": [
        "train=dataset['train']\n",
        "for i in range(0, 200, 5):\n",
        "  print(train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqfiYIlTGX2I"
      },
      "source": [
        "test_input=data['document'][i] # 1st input text\n",
        "print(test_input)\n",
        "type(test_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN2CBQnp5pEF"
      },
      "source": [
        "## Load pre-trained model\n",
        "\n",
        "Next, we'll load the pre-trained model (especially the weights!) with support for text classification output using [`AutoModelForSequenceClassification.from_prertained`](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModelForSequenceClassification.from_pretrained).\n",
        "\n",
        "Note that we need to provide the number of labels in the data when loading the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNZ_8gDxGMcS"
      },
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xixsVaZRvyau"
      },
      "source": [
        "TODO: Training and validation accuracy (some fancy plots maybe) --> choose the best model (best hypearparameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1G_VqkHwG-L"
      },
      "source": [
        "# MS 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUesGElhwL2S"
      },
      "source": [
        "## test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJfKgi1Xs5k7"
      },
      "source": [
        "precision and recall, F-score"
      ]
    }
  ]
}